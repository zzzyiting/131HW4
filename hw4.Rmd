---
title: "Untitled"
author: "Yiting Zhang"
date: '2022-05-02'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

```{r}
# load the libraries
library(tidymodels)
library(ggplot2)
library(discrim)
library(corrr)
library(klaR)
library(caret)
library(ggplot2)
library(tidyverse)
library(corrplot)
library(ggthemes)
library(cli)
library(recipes)
library(pROC)
library(yardstick)
library(MASS)
library(poissonreg)
library(naivebayes)
tidymodels_prefer()

# load and factor data.
titanic <- read.csv(file = 'titanic.csv')
titanic$survived <- factor(titanic$survived, levels = c("Yes","No"))
titanic$pclass <- factor(titanic$pclass)
```

### Question 1

Split the data, stratifying on the outcome variable, `survived.`  You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. 


```{r}
set.seed(100)
titanic_split <- initial_split(titanic, prop = 0.80,
                                strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)



data<-dim(titanic)[1]
train<-dim(titanic_train)[1]
test<-dim(titanic_test)[1]
train/data
test/data


titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>%
  step_impute_linear(age) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~ starts_with("sex"):fare + age:fare)
```
we can verify the number of observations was split correctly.

### Question 2

Fold the **training** data. Use *k*-fold cross-validation, with $k = 10$.

```{r}
titanic_folds <- vfold_cv(titanic_train, v = 10)
titanic_folds
```


### Question 3

In your own words, explain what we are doing in Question 2. What is *k*-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we **did** use the entire training set, what resampling method would that be?


One of the resampling method is k-fold cross-validation. The training data is partitioned at random into sets of equal size which are called folds. For each iteration of resampling in 10-fold cross validation, one fold is kept as an assessment set to evaluate the model, and the remaining 9 folds are utilized as an analysis set to fit the model. The averages of each iteration make up the final resampling estimate of model performance. Because the model is developed based on the training data set, just fitting and testing models on the training set will result in very good performance. And the validation set approach would be used if we used the whole training set for resampling.


### Question 4

Set up workflows for 3 models:

1. A logistic regression with the `glm` engine;
2. A linear discriminant analysis with the `MASS` engine;
3. A quadratic discriminant analysis with the `MASS` engine.

How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you'll fit to each fold.



```{r}
# logistic regression with glm engine
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)
```

```{r}
# linear discriminant analysis with MASS engine
lda_mod <- discrim_linear() %>% 
  set_engine("MASS") %>% 
  set_mode("classification")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)
```


```{r}
# a quadratic discriminant analysis with MASS engine
qda_mod  <- discrim_quad() %>% 
  set_engine("MASS") %>% 
  set_mode("classification")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)
```

There will be 30 models I am fitting, there are 10 folds for each engine and there are 3 different engine. 

### Question 5

Fit each of the models created in Question 4 to the folded data.
```{r}
# logistic regression
log_res <- log_wkflow %>% 
  fit_resamples(resamples = titanic_folds)
```

```{r}
# linear discriminant
lda_res <- lda_wkflow %>% 
  fit_resamples(resamples = titanic_folds)
```

```{r}
# quadratic discriminant
qda_res <- qda_wkflow %>% 
  fit_resamples(resamples = titanic_folds)
```



### Question 6

Use `collect_metrics()` to print the mean and standard errors of the performance metric *accuracy* across all folds for each of the four models.

Decide which of the 3 fitted models has performed the best. Explain why. *(Note: You should consider both the mean accuracy and its standard error.)*

```{r}
log_acc <- collect_metrics(log_res)
log_acc
#95% confidence interval
log_acc$mean[1] - 1.96*sqrt(log_acc$std_err[1]/10)
log_acc$mean[1] + 1.96*sqrt(log_acc$std_err[1]/10)
```



```{r}
lda_acc <- collect_metrics(lda_res)
lda_acc
#95% confidence interval
lda_acc$mean[1] - 1.96*sqrt(lda_acc$std_err[1]/10)
lda_acc$mean[1] + 1.96*sqrt(lda_acc$std_err[1]/10)
```


```{r}
qda_acc <- collect_metrics(qda_res)
qda_acc
#95% confidence interval
qda_acc$mean[1] - 1.96*sqrt(qda_acc$std_err[1]/10)
qda_acc$mean[1] + 1.96*sqrt(qda_acc$std_err[1]/10)
```



```{r}
mean_accuracy <- c(log_acc$mean[1],lda_acc$mean[1], qda_acc$mean[1])
Standard_error <- c(log_acc$std_err[1],lda_acc$std_err[1], qda_acc$std_err[1])
models <- c("Logistic Regression", "LDA", "QDA")
results <- tibble(accuracies = mean_accuracy, Standard_error = Standard_error, models = models)
results %>% 
  arrange(-accuracies)
```

logistic model has the best performance in this example it has highest mean accuracy. The logistic regression also have the highest lower bound and upper bound.




### Question 7

Now that you’ve chosen a model, fit your chosen model to the entire training dataset (not to the folds).

```{r}
log_fit <- fit(log_wkflow, titanic_train)
log_fit %>% tidy()
```


### Question 8

Finally, with your fitted model, use `predict()`, `bind_cols()`, and `accuracy()` to assess your model’s performance on the testing data!

Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.


```{r}
predict(log_fit, new_data = titanic_test, type = "prob")

log_reg_acc <- augment(log_fit, new_data = titanic_test) %>%
  accuracy(truth = survived, estimate = .pred_class)
x <- collect_metrics(log_res)[1,][1:3]
names(x)[3] <- '.estimate'
bind_rows(log_reg_acc,x)%>%
add_column(models=c('testing','folds'), .before = ".metric")
```


What I see is that the model's testing accuracy is a little bit higher than the average accuracy across folds.